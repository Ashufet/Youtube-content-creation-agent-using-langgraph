{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Required Packages","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade youtube_search youtube-transcript-api pytube --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-21T11:43:38.301555Z","iopub.status.idle":"2024-03-21T11:43:53.044814Z","shell.execute_reply.started":"2024-03-21T11:43:38.301875Z","shell.execute_reply":"2024-03-21T11:43:53.043782Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install langchain langchain_community langgraph tiktoken --quiet","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:43:53.046891Z","iopub.execute_input":"2024-03-21T11:43:53.047214Z","iopub.status.idle":"2024-03-21T11:44:11.293296Z","shell.execute_reply.started":"2024-03-21T11:43:53.047186Z","shell.execute_reply":"2024-03-21T11:44:11.292368Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub --quiet","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:44:11.294521Z","iopub.execute_input":"2024-03-21T11:44:11.294800Z","iopub.status.idle":"2024-03-21T11:44:24.941427Z","shell.execute_reply.started":"2024-03-21T11:44:11.294763Z","shell.execute_reply":"2024-03-21T11:44:24.940139Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# LLM Inference using HuggingFaceEndpoint","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import HuggingFaceEndpoint\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nHUGGINGFACEHUB_API_TOKEN = user_secrets.get_secret(\"hf_model\")\n\nrepo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n\nllm = HuggingFaceEndpoint(\n    repo_id=repo_id, max_length=1024, temperature=0.1, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:49:26.532253Z","iopub.execute_input":"2024-03-21T11:49:26.532633Z","iopub.status.idle":"2024-03-21T11:49:26.764411Z","shell.execute_reply.started":"2024-03-21T11:49:26.532602Z","shell.execute_reply":"2024-03-21T11:49:26.763392Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating YouTube Script Creation Agent using LangGraph","metadata":{}},{"cell_type":"code","source":"from typing import Dict, TypedDict\n\nfrom langchain_core.messages import BaseMessage\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        keys: A dictionary where each key is a string.\n    \"\"\"\n\n    keys: Dict[str, any]","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:49:50.263945Z","iopub.execute_input":"2024-03-21T11:49:50.264362Z","iopub.status.idle":"2024-03-21T11:49:50.269620Z","shell.execute_reply.started":"2024-03-21T11:49:50.264311Z","shell.execute_reply":"2024-03-21T11:49:50.268579Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from langchain.tools import YouTubeSearchTool\nfrom langchain_community.document_loaders import YoutubeLoader\n\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.prompts import PromptTemplate\n\n### Nodes ###\ndef get_youtube_video_urls(state):\n    print(\"---Fetching URls---\")\n    state_dict = state[\"keys\"]\n    question = state_dict[\"question\"]\n    top_k = state_dict[\"top_k\"]\n    \n    tool = YouTubeSearchTool()\n    if \"docs\" in state_dict:\n        print(\"---GETTING NEW VIDEO URLS---\", top_k+top_k)\n        url_list = tool.run(question+\",\"+str(top_k+top_k))\n    else:\n        url_list = tool.run(question+\",\"+str(top_k))\n    \n    final_urls = []\n    for u in url_list.split(\"'\"):\n        if \"http\" in u:\n            final_urls.append(u)\n    return {\"keys\": {\"url_list\":final_urls, \"question\":question, \"top_k\":top_k}}\n\n\ndef get_video_text_from_urls(state):\n    print(\"---Fetching Text from Video---\")\n    state_dict = state[\"keys\"]\n    question = state_dict[\"question\"]\n    top_k = state_dict[\"top_k\"]\n    final_urls = state_dict[\"url_list\"]\n    docs = []\n    for url in final_urls:\n        loader = YoutubeLoader.from_youtube_url(\n                        url,\n                        add_video_info=True,\n                        language=[\"en\",\"hi\"],\n                        translation=\"en\",\n                        )\n        text = loader.load()\n        if len(text)>0:\n            docs.append(text[0])\n    \n    return {\"keys\": {\"docs\":docs, \"url_list\":final_urls, \"question\":question, \"top_k\":top_k}}\n\n\ndef get_text_summary(state):\n    print(\"---Creating Video Text Summary---\")\n    state_dict = state[\"keys\"]\n    question = state_dict[\"question\"]\n    final_urls = state_dict[\"url_list\"]\n    top_k = state_dict[\"top_k\"]\n    docs = state_dict[\"docs\"]\n    \n    prompt_template = \"\"\"Write a detailed summary highlighting key points to engage users of the following text:\n    {text}\n    Detailed Summary:\"\"\"\n    prompt = PromptTemplate.from_template(prompt_template)\n\n    refine_template = (\n        \"Your job is to produce a final detailed summary which should include the existing summary and only include \"\n        \"relevant new key points which can improve the summary from the 'new context' provided. \\n\"\n        \"We have provided an 'existing summary' up to a certain point: Existing summary - \\n {existing_answer}\\n\"\n        \"You have the opportunity to refine the existing summary, make sure to add only new points \"\n        \"(only if needed) with some more context below. Don't remove key points from existing summary \\n\"\n        \"New Context - \\n\"\n        \"------------\\n\"\n        \"{text}\\n\"\n        \"------------\\n\"\n        \"Given the new context, refine the existing summary in English detailing all key points\"\n        \"If the context isn't useful, return the Existing summary.\"\n    )\n    refine_prompt = PromptTemplate.from_template(refine_template)\n    chain = load_summarize_chain(\n        llm=llm,\n        chain_type=\"refine\",\n        question_prompt=prompt,\n        refine_prompt=refine_prompt,\n        return_intermediate_steps=True,\n        input_key=\"input_documents\",\n        output_key=\"output_text\",\n    )\n    result = chain({\"input_documents\": docs}, return_only_outputs=True)\n    \n    return {\"keys\": {\"generated_summary\":result[\"output_text\"], \"generated_intermediate_summary\":result[\"intermediate_steps\"], \"docs\":docs, \"url_list\":final_urls, \"question\":question, \"top_k\":top_k}}\n\n\n### Edges ###\ndef decide_to_get_summary(state):\n    \"\"\"\n    Determines whether to generate the summary, or get new video urls.\n\n    Args:\n        state (dict): The current state of the agent, including all keys.\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    print(\"---DECIDE TO GET SUMMARY---\")\n    \n    state_dict = state[\"keys\"]\n    docs = state_dict[\"docs\"]\n\n    if len(docs)>=2:\n        # We have relevant documents, so generate summary\n        print(\"---DECISION: GET TEXT SUMMARY---\")\n        return \"get_text_summary\"\n    else:\n        # We have no relevant documents, so get new video urls\n        print(\"---DECISION: GET NEW VIDEO URLS---\")\n        return \"get_youtube_video_urls\"","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:49:51.855468Z","iopub.execute_input":"2024-03-21T11:49:51.856273Z","iopub.status.idle":"2024-03-21T11:49:52.026883Z","shell.execute_reply.started":"2024-03-21T11:49:51.856243Z","shell.execute_reply":"2024-03-21T11:49:52.026139Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Build Graph\n\nThis just follows the flow we outlined in the figure above.","metadata":{}},{"cell_type":"code","source":"import pprint\n\nfrom langgraph.graph import END, StateGraph\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"get_youtube_video_urls\", get_youtube_video_urls)\nworkflow.add_node(\"get_video_text_from_urls\", get_video_text_from_urls)\nworkflow.add_node(\"get_text_summary\", get_text_summary)\n\n# Build graph\nworkflow.set_entry_point(\"get_youtube_video_urls\")\nworkflow.add_edge(\"get_youtube_video_urls\", \"get_video_text_from_urls\")\nworkflow.add_conditional_edges(\n    \"get_video_text_from_urls\",\n    decide_to_get_summary,\n    {\n        \"get_text_summary\": \"get_text_summary\",\n        \"get_youtube_video_urls\": \"get_youtube_video_urls\",\n    },\n)\nworkflow.add_edge(\"get_text_summary\", END)\n\n# Compile\napp = workflow.compile()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:49:54.427609Z","iopub.execute_input":"2024-03-21T11:49:54.428300Z","iopub.status.idle":"2024-03-21T11:49:54.475244Z","shell.execute_reply.started":"2024-03-21T11:49:54.428266Z","shell.execute_reply":"2024-03-21T11:49:54.474465Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# RUN","metadata":{}},{"cell_type":"code","source":"# Run\ninputs = {\"keys\": {\"question\": \"What is retrieval augmented generation\",\"top_k\":3}}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        # Node\n        pprint.pprint(f\"Node '{key}':\")\n    pprint.pprint(\"\\n---\\n\")\n\n# Final generation\npprint.pprint(value['keys']['generated_summary'])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:49:56.645228Z","iopub.execute_input":"2024-03-21T11:49:56.645940Z","iopub.status.idle":"2024-03-21T11:50:42.054712Z","shell.execute_reply.started":"2024-03-21T11:49:56.645908Z","shell.execute_reply":"2024-03-21T11:50:42.053687Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"---Fetching URls---\n\"Node 'get_youtube_video_urls':\"\n'\\n---\\n'\n---Fetching Text from Video---\n\"Node 'get_video_text_from_urls':\"\n'\\n---\\n'\n---DECIDE TO GET SUMMARY---\n---DECISION: GET TEXT SUMMARY---\n---Creating Video Text Summary---\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"name":"stdout","text":"\"Node 'get_text_summary':\"\n'\\n---\\n'\n\"Node '__end__':\"\n'\\n---\\n'\n('\\n'\n '\\n'\n 'Retrieval-Augmented Generation (RAG) is a crucial framework for enhancing '\n 'the performance of Large Language Models (LLMs) by incorporating a content '\n 'store for retrieving relevant information before generating a response. The '\n 'new context emphasizes the use of RAG in creating a better user experience '\n \"by leveraging LLMs on a user's own content, rather than just listing \"\n 'relevant content like a search engine. RAG is used to create a chatbot or a '\n \"system that can answer questions based on a user's specific content source, \"\n 'such as a website or a library of PDF documents.\\n'\n '\\n'\n 'The RAG architecture consists of three main components: the user, the '\n \"content source, and the large language model (LLM). The user's question is \"\n 'bundled into a prompt and sent to the LLM, which generates a response. To '\n 'enhance the user experience, RAG uses prompt engineering, where instructions '\n 'and information from the content source are added to the prompt before it is '\n 'sent to the LLM. This allows the LLM to generate a more accurate and '\n \"relevant response based on the user's specific content source.\\n\"\n '\\n'\n 'To ensure that the LLM uses only the relevant parts of the content source, '\n 'RAG breaks the content into chunks and turns each chunk into a vector. '\n 'Similar paragraphs on similar topics will have similar vectors, allowing the '\n 'system to find the most relevant content quickly. When a user asks a '\n 'question, the system compares the vector of the question to the vectors of '\n 'the content and retrieves the most relevant documents. The retrieved '\n 'paragraphs are then used as part of the prompt before it is sent to the LLM, '\n \"ensuring that the LLM's response is accurate and relevant to the user's \"\n 'question.\\n'\n '\\n'\n 'In summary, RAG is a crucial framework for improving the accuracy and '\n 'timeliness of LLMs, particularly in chatbot applications. It helps address '\n 'issues like hallucination and biases by incorporating a content store for '\n 'retrieving relevant information and ensuring that the response is up-to-date '\n 'and based on primary source data. Additionally, RAG is a form of prompt '\n 'engineering, where context is added to the prompt to obtain more accurate '\n 'and relevant responses from the LLM, and it allows for the creation of a '\n \"better user experience by leveraging LLMs on a user's own content.\")\n","output_type":"stream"}]},{"cell_type":"code","source":"value['keys']","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:51:28.228367Z","iopub.execute_input":"2024-03-21T11:51:28.229143Z","iopub.status.idle":"2024-03-21T11:51:28.235964Z","shell.execute_reply.started":"2024-03-21T11:51:28.229111Z","shell.execute_reply":"2024-03-21T11:51:28.235102Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'generated_summary': \"\\n\\nRetrieval-Augmented Generation (RAG) is a crucial framework for enhancing the performance of Large Language Models (LLMs) by incorporating a content store for retrieving relevant information before generating a response. The new context emphasizes the use of RAG in creating a better user experience by leveraging LLMs on a user's own content, rather than just listing relevant content like a search engine. RAG is used to create a chatbot or a system that can answer questions based on a user's specific content source, such as a website or a library of PDF documents.\\n\\nThe RAG architecture consists of three main components: the user, the content source, and the large language model (LLM). The user's question is bundled into a prompt and sent to the LLM, which generates a response. To enhance the user experience, RAG uses prompt engineering, where instructions and information from the content source are added to the prompt before it is sent to the LLM. This allows the LLM to generate a more accurate and relevant response based on the user's specific content source.\\n\\nTo ensure that the LLM uses only the relevant parts of the content source, RAG breaks the content into chunks and turns each chunk into a vector. Similar paragraphs on similar topics will have similar vectors, allowing the system to find the most relevant content quickly. When a user asks a question, the system compares the vector of the question to the vectors of the content and retrieves the most relevant documents. The retrieved paragraphs are then used as part of the prompt before it is sent to the LLM, ensuring that the LLM's response is accurate and relevant to the user's question.\\n\\nIn summary, RAG is a crucial framework for improving the accuracy and timeliness of LLMs, particularly in chatbot applications. It helps address issues like hallucination and biases by incorporating a content store for retrieving relevant information and ensuring that the response is up-to-date and based on primary source data. Additionally, RAG is a form of prompt engineering, where context is added to the prompt to obtain more accurate and relevant responses from the LLM, and it allows for the creation of a better user experience by leveraging LLMs on a user's own content.\",\n 'generated_intermediate_summary': ['\\n    - Large language models (LLMs) are widely used but can sometimes generate incorrect or outdated information.\\n    - The speaker, Marina Danilevsky, a Senior Research Scientist at IBM Research, introduces a framework called Retrieval-Augmented Generation (RAG) to improve the accuracy and timeliness of LLMs.\\n    - The \"Generation\" part of RAG refers to LLMs that generate text in response to a user query, also known as a prompt.\\n    - An example of an LLM\\'s undesirable behavior is when the speaker confidently answered her kids\\' question about the planet with the most moons by saying Jupiter, but without citing a source and with outdated information.\\n    - RAG addresses two common LLM challenges: lack of source and outdated information.\\n    - The \"Retrieval-Augmented\" part of RAG involves adding a content store, which can be open (e.g., the internet) or closed (e.g., a collection of documents or policies), to the LLM.\\n    - The LLM first retrieves relevant content from the content store and then generates a response based on the user\\'s query and the retrieved content.\\n    - RAG helps ensure that the LLM\\'s response is up to date and based on primary source data, reducing the likelihood of hallucination or data leakage.\\n    - RAG also enables the LLM to say \"I don\\'t know\" when it cannot reliably answer the user\\'s query based on the content store.\\n    - Improving both the retriever and the generative part of the LLM is essential to ensure the best quality data for grounding the response and providing the richest, most accurate answer to the user.',\n  '\\n\\nRefined Summary -\\n\\nThe existing summary provides a good overview of Retrieval-Augmented Generation (RAG), a framework that enhances the accuracy and timeliness of Large Language Models (LLMs) by incorporating a content store for retrieving relevant information before generating a response. The new context further emphasizes the importance of RAG in addressing issues like hallucination and biases in LLMs, particularly in chatbot applications.\\n\\nHallucination in LLMs refers to the generation of random, seemingly accurate information that is not based on actual knowledge. RAG helps mitigate this issue by automatically adding more knowledge or content into the interaction with an LLM, ensuring that the response is up-to-date and based on primary source data.\\n\\nThe new context also highlights that RAG is as important as the coursebook for success in AI, and understanding its use cases is highly relevant for those working with LLMs or large language models. RAG is a form of prompt engineering, where context is added to the prompt to obtain more accurate and relevant responses from the LLM.\\n\\nIn summary, RAG is a crucial framework for improving the accuracy and timeliness of LLMs, particularly in chatbot applications. It helps address issues like hallucination and biases by incorporating a content store for retrieving relevant information and ensuring that the response is up-to-date and based on primary source data. Additionally, RAG is a form of prompt engineering, where context is added to the prompt to obtain more accurate and relevant responses from the LLM.',\n  \"\\n\\nRetrieval-Augmented Generation (RAG) is a crucial framework for enhancing the performance of Large Language Models (LLMs) by incorporating a content store for retrieving relevant information before generating a response. The new context emphasizes the use of RAG in creating a better user experience by leveraging LLMs on a user's own content, rather than just listing relevant content like a search engine. RAG is used to create a chatbot or a system that can answer questions based on a user's specific content source, such as a website or a library of PDF documents.\\n\\nThe RAG architecture consists of three main components: the user, the content source, and the large language model (LLM). The user's question is bundled into a prompt and sent to the LLM, which generates a response. To enhance the user experience, RAG uses prompt engineering, where instructions and information from the content source are added to the prompt before it is sent to the LLM. This allows the LLM to generate a more accurate and relevant response based on the user's specific content source.\\n\\nTo ensure that the LLM uses only the relevant parts of the content source, RAG breaks the content into chunks and turns each chunk into a vector. Similar paragraphs on similar topics will have similar vectors, allowing the system to find the most relevant content quickly. When a user asks a question, the system compares the vector of the question to the vectors of the content and retrieves the most relevant documents. The retrieved paragraphs are then used as part of the prompt before it is sent to the LLM, ensuring that the LLM's response is accurate and relevant to the user's question.\\n\\nIn summary, RAG is a crucial framework for improving the accuracy and timeliness of LLMs, particularly in chatbot applications. It helps address issues like hallucination and biases by incorporating a content store for retrieving relevant information and ensuring that the response is up-to-date and based on primary source data. Additionally, RAG is a form of prompt engineering, where context is added to the prompt to obtain more accurate and relevant responses from the LLM, and it allows for the creation of a better user experience by leveraging LLMs on a user's own content.\"],\n 'docs': [Document(page_content='Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name\\xa0is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want\\xa0to tell you about a framework to help large language models be more accurate and more up to\\xa0date: Retrieval-Augmented Generation, or RAG. Let\\'s just talk about the \"Generation\" part for a\\xa0minute. So forget the \"Retrieval-Augmented\". So the\\xa0generation, this refers to large language models,\\xa0or LLMs, that generate text in response to a user query, referred to as a prompt. These\\xa0models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question: \"In our solar system, what planet has the most\\xa0moons?\" And my response was, “Oh, that\\'s really great that you\\'re asking this question. I loved\\xa0space when I was your age.” Of course, that was like 30 years ago. But I know this! I read an\\xa0article and the article said that it was Jupiter and 88 moons. So that\\'s the answer. Now, actually,\\xa0there\\'s a couple of things wrong with my answer. First of all, I have no source to support what\\xa0I\\'m saying. So even though I confidently said “I read an article, I know the answer!”, I\\'m not\\xa0sourcing it. I\\'m giving the answer off the top of my head. And also, I actually haven\\'t kept up with\\xa0this for awhile, and my answer is out of date. So we have two problems here. One is no source.\\xa0And the second problem is that I am out of date.\\xa0\\xa0 And these, in fact, are two behaviors that are\\xa0often observed as problematic when interacting with large language models. They’re LLM\\xa0challenges. Now, what would have happened if I\\'d taken a beat and first gone and looked\\xa0up the answer on a reputable source like NASA? Well, then I would have been able to say, “Ah,\\xa0okay! So the answer is Saturn with 146 moons.” And in fact, this keeps changing because scientists\\xa0keep on discovering more and more moons. So I have now grounded my answer in something more\\xa0 believable. I have not hallucinated or made up an answer. Oh, by the way, I didn\\'t leak personal\\xa0information about how long ago it\\'s been since I was obsessed with space. All right, so what does\\xa0this have to do with large language models? Well, how would a large language model have answered\\xa0this question? So let\\'s say that I have a user asking this question about moons. A large language\\xa0model would confidently say, OK, I have been trained and from what I know in my parameters\\xa0during my training, the answer is Jupiter. The answer is wrong. But, you know, we don\\'t know. The large language model is very confident in what it answered. Now, what happens when you add this\\xa0retrieval augmented part here? What does that mean? That means that now, instead of just relying\\xa0on what the LLM knows, we are adding a content store. This could be open like the internet. This\\xa0can be closed like some collection of documents, collection of policies, whatever. The point,\\xa0though, now is that the LLM first goes and talks to the content store and says,\\xa0“Hey, can you retrieve for me information that is relevant to what the user\\'s\\xa0query was?” And now, with this retrieval-augmented answer, it\\'s not Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and retrieve the most up to date information. The second problem, source. Well, the large language model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, “I don\\'t know.” If\\xa0the user\\'s question cannot be reliably answered based on your data store, the model should say,\\xa0\"I don\\'t know,\" instead of making up something that is believable and may mislead the user. This\\xa0can have a negative effect as well though, because if the retriever is not sufficiently\\xa0good to give the large language model the best, most high-quality grounding information, then\\xa0maybe the user\\'s query that is answerable doesn\\'t get an answer. So this is actually why lots\\xa0of folks, including many of us here at IBM, are working the problem on both sides. We are both\\xa0working to improve the retriever to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you. \\n', metadata={'source': 'T-D1OfcDW1M', 'title': 'What is Retrieval-Augmented Generation (RAG)?', 'description': 'Unknown', 'view_count': 330824, 'thumbnail_url': 'https://i.ytimg.com/vi/T-D1OfcDW1M/hq720.jpg', 'publish_date': '2023-08-23 00:00:00', 'length': 395, 'author': 'IBM Technology'}),\n  Document(page_content=\"when using chat GPT you most probably  have encountered responses like I'm  sorry but as of my last knowledge update  in January 2022 Etc or even responses  that are not true at all this is where  rag comes into play and says let me help  you by injecting more knowledge or  content into your interactions with an  llm and help it answer the unknown and  upcoming questions we hear llms prompts  and rag Everywhere by now I think most  of us know what an llm and the prompt is  but did you know that right now rag is  just as important as both of these and  Powers most applications you may use  involving a chatbot I recently did a  poll on the learn AI together Discord  Community to find out if people had  already studied created or used rag  applications and most voted to  understand what rag is used for rag is  as important as your coursebook for  success in a class so understanding what  it is is highly relevant in AI an llm or  a large language mode model is just an  AI model trained on language to talk  with humans like GPT 4 used in ch GPT a  prompt is simply your interaction with  it it's the question you ask it but if  you are experiencing issues like  hallucinations or biases using such a  language model or llm then rag or  retrieval augmented Generations comes  into play Let's quickly clarify  hallucinations first it's when the model  returns random things that seems true  but aren't simply because it doesn't  know the answer in fact a language model  is constantly hallucinating it only  predicts words in a statistical way it  turns out that when they are trained  with the entire internet there are so  many examples that they manage to  accurately predict the next logical  words to answer most questions despite  this it hallucinates it doesn't really  understand what it's talking about and  just outputs one word at a time that is  probable what is incredible is that most  of these hallucinations are actually  true and answer our questions however  some of them are real hallucinations of  fabricated facts or scenarios and that  can cause quite a few problems if they  are not sufficiently controlled while  there are several reasons why llms  hallucinate it is mostly because they  lack relevant context either because  they cannot find the relevant data or  don't know which data to refer to for a  particular question this is because they  were trained to answer and not to say I  don't know rag solves this by  automatically adding more knowledge or  content into your interactions with an  llm put simply you have a data set which  is required and you use it to help the  llm answer the unknown and upcoming user  questions this is the simplest form and  requires a few steps to make it work but  this is the gist of a rag based system  you have a user question that is sent to  an automatic search in the database for  finding relevant information which is  then used back along with the question  to give back an answer to the user as  you can see with frag we use context  from the user question question and our  knowledge base to answer it this helps  with grounding our model to the  knowledge we control making it safer and  aligned the disadvantage is limiting our  answers to our knowledge base which is  finite and probably not as big as the  Internet it's just like an open book  exam you would have in school you  already have access to most answers and  simply need to know where it is in your  knowledge base if you find the answer in  the manual it's quite hard to fail the  question and write something wrong Jerry  Leu CEO of L index gave a very  interesting view on how to see rag in my  most recent podcast with him if you  think about it rag is basically prompt  engineering because you're basically  figuring out a way to put context into  the prompt uh it's just a programmatic  way of prompt engineering it's a way of  prompting so that you actually get back  um some contacts he also said to  subscribe to the channel to learn more  about AI okay maybe that's just a  hallucination actually but you should  still do it honestly in rag you first  need data or knowledge which can be in  the form of documentation but books  articles Etc and only allow the llm to  search and respond if the answer to the  question is inside this knowledge base  you have anyways if you have access to  accurate information in your school  manual why would you try to come up with  something different Instead This is  currently the best way to control your  outputs and make your model safer and  aligned basically the best way to ensure  you will give the right answer and get  your best grade for example we recently  built an AI tutor to answer AI related  questions we wanted accurate responses  for our students both in terms of  accuracy to give the right answer and in  terms of relevancy so upto-date  information with rag you can simply  update your database if things have  changed there's no big deal if the whole  pytorch Library had a big update  yesterday scrape it again and update  your data set in a second and voila you  don't have to retrain the whole model or  wait for a gp4 to finally update the  noledge card update the overall process  of the butt is quite straightforward we  validate the question answering it is  related to Ai and that our chatbot  should answer it then we search in our  database to find good and relevant  sources and finally use chat GPT to  digest those sources and give a good  answer for the student if you need safe  information from an AI chat but like a  medical assistant a tutor a lawyer or an  accent you will be using rag for sure  well maybe not if you're listening in  2030 but as of now rag is by far the  best and safest approach to using a  chatbot where you need to give factual  and accurate information to build a rag  based chatbot or application like our AI  tutor we start by ingesting all our data  into memory this is done by splitting  all the content into chunks of text so  split our textual data into fixed or  flexible parts for example 500 character  parts and processing it to an embedding  model like open AI text embedding Adda  model this will produce embeddings that  are just vectors of numbers repres  representing your text it will  facilitate your life and allow you to  compare text together easily you can  save those vectors in a memory then for  a new question from a particular user  you can repeat this process and answer  this means embedding the question using  the same approach and compare it with  all your current embeddings in your  memory here you are basically looking  for the most probable answer for this  question searching in your memory just  like you do for an exam looking through  the chapters to find a title that seems  relevant to the current exam question  once it finds the most similar embedding  chat GPT is asked to understand the  user's question and intent and only use  the retrieved sources of knowledge to  answer the question this is how rag  reduces hallucination risks and allows  you to have upto-date information since  you can update your knowledge base as  much as you want and chat gbt or your  current language model simply picks  information from it to answer plus as  you see it cites all sources it found on  question for you to dive in and learn  more which is also a plus when you're  trying to learn and understand a new  topic then there are still many things  to consider like how to determine when  to answer a question or not if it is  relevant or in your  documentation understand new terms or  acronyms not in chat gpt's knowledge  base find the relevant information more  efficiently and accurately etc those  concerns are all things we've improved  through using various techniques like  better chunking methods rank ERS query  expansion agents and more that you can  learn about in our free Advanced rag  course we've built together with toi and  active Loop that I Linked In the  description below before some of you may  ask yes an alternative to rag would be  to fine-tune your model on your specific  task basically to further train a model  on your own data to make it more  specific and ingest the knowledge you  have rather than always searching in it  like memorizing the book before the exam  instead of bringing it with you I have a  video comparing fine tuning and rag to  teach you when you should consider each  but in short rag stays relevant with or  without fine tuning as it is much  cheaper to build and is better to reduce  undesired hallucinations as you force  the model to give answers based on  documentation you control and not simply  things it ingested and hopefully will  regurgitate correctly as in fine tune  models coming back to our open book  exams is just like professors making you  focus on understanding the core matter  and logic and not the knowledge itself  as you can always find it in manuals or  on Google same here for llms and  complimenting them with rag plus even  though those models have much better  memories than us they are not perfect  and they will not retain all the  information you give them thus even with  a fine tune model on hyp specific data  rag remains something worth leveraging  before we end this video I just wanted  to mention that we discussed both these  Topics in depth with coding examples in  our llm and rag courses if you want to  put this knowledge into practice the  link is in the description below and  they are completely free I hope you've  enjoyed this video and that it helps you  understand the goals and principles of  rag better if you did please share it  with a friend or your network to spread  the know Edge and help the channel grow  thank you for  [Music]  watching  [Music]\", metadata={'source': 'LAfrShnpVIk', 'title': 'What is Retrieval Augmented Generation (RAG) - Augmenting LLMs with a memory', 'description': 'Unknown', 'view_count': 13832, 'thumbnail_url': 'https://i.ytimg.com/vi/LAfrShnpVIk/hq720.jpg', 'publish_date': '2024-01-09 00:00:00', 'length': 580, 'author': \"What's AI by Louis-François Bouchard\"}),\n  Document(page_content=\"hello everyone uh welcome to my code  deare uh video series um what I'm doing  is I'm rotating through three different  types of topics educational topics uh  use case topics and then kind of bias  ethics safety uh topic so now on the  education rotation and today what I  wanted to talk about is uh what is  retrieval augmented generation or rag uh  and you may think that I'm going into  some kind of nook and cranny of the AI  uh field but this is a very important  and popular kind of solution pattern  that I see um being used over and over  and over again for uh how to leverage  large language models so I thought I  would explain it uh to you uh and the  the the thing that this is used for is  basically systems that leverage large  language models but on your own content  so let me describe that if you think of  like the chat GPT experience and if you  think about that um relative to like the  search engine experience that we had  before if you ask a question like um I  don't know what color is the sky or how  do I fix this plumbing issue or  something like that a search engine  would go out uh or appear to go out  search the internet find relevant  content and then just list that content  for you list those links and then you as  a user would need to click on the links  that seem seem right read it digest it  and figure out the answer to your  question what a large language model  does is it seems to do that first part  meaning leverage the content on the  whole internet but instead of just  listing that content it sort of digests  it digests it combines it assembles it  together and answers your question sort  of generates an answer um so it's a  whole lot better I mean search engines  have been great but this is taking the  whole experience to another level and in  addition the question and answering uh  you can also give it instructions like  write me this document or write me a  lesson plan to teach geometry to seventh  graders uh and it will do something  similar it will kind of assemble content  that it SE that it has seen uh that  talks about geometry or seventh graders  or how to do lesson plans or whatever uh  pulls that together assembles it and  then writes out a lesson plan okay so  it's a much better experience than just  taking the raw content from the internet  but it really uh creates something new  from that now let's say you want that  same experience but on your own content  so it might be a chatbot on your website  or you might have a library of PDF  documents that this documentation for  one of your products uh and instead of  just linking the user to parag sections  of the documentation you want to  actually answer their question uh it  might be your service ticketing uh  system so when a new issue comes in you  could say how would I resolve this issue  and it can assemble past similar issues  uh and then come up with a new uh new  solution based on that so this is an  incredible experience that these large  language models offer but how can you  create that experience on your own  content uh that might not be available  to the internet or available to these  large language models well the solution  to this is this rag um architecture this  retrieval augmented uh generation  architecture so now I'm going to do my  best to explain that uh to  you so let's say you have a um  user and I'm going to use the example of  a uh patient chatbot and the content  source is going to be that content from  your website let's say or could be  content from PDF documents or or  whatever but you want this to be the  content to answer the patient's  questions so if the patient has a  question like how do I prepare for my  knee surgery instead of just going to  chat sheet PT and getting a generic  answer you'd like to provide an answer  that's from your health system or a  question like do you have parking you'd  like to provide an answer for your  health system for your the office where  the patient is seen okay so that's a  scenario that I'd like to do so the  patient has a  question uh and I'm going to do do you  have  parking have  parking um you can uh imagine that  question being bundled up into a prompt  what's called a prompt and I'll describe  this more  later so there is the question that  prompt is sent to a large language  model and that large language model will  come up with a response to that question  okay now um if you just wanted to use uh  chat GPT let's say or some other llm uh  without any extra content you could just  use this flow how do I prepare for my  knee surgery or do you have parking put  that into a prompt send that to the uh  large language model and get a response  back okay but uh but what we want to do  is enhance this experience with our own  content so let's say here is your  content  source and again this might be all the  content of your website or PDF documents  or internal ticketing system or  databases or that uh that sort of thing  and what you'd like to do is something  called called the prop before the propt  so in these systems you don't just send  the user question to the large language  model you usually have some level of  instructions So the instructions might  be you are a contact center specialist  working for a hospital answering patient  questions that come in over the Internet  uh please be uh nice to the patients and  responsive and folksy because that fits  with our brand or some instructions like  that are sometimes sent with the prompt  um and then uh  Additionally you want to provide the  information that the L llm needs to  answer the question so what you'd  ideally like is information from your  website to be included here um and uh  and that to be sent to the llm as well  so the full prompt might be your  instructions it might be something like  please use this content um in order to  answer the patient question at the end  and then you put in a bunch of  information about parking or about knee  surgery or whatever the patient asked  you put that in the prompt before the  prompt then you have the question then  you send that whole package to the llm  and the llm will give a great response  based on your  content okay with me so far so um so  this notion is the prop before the  prompt um and and that's why prompt  engineering and these types of things  are a big field right now now because  you can really hone the um these systems  by doing a better and better job with  the actual prompt before the prompt um  in uh in this  style now the last trick here is your  website or your content is huge and it  talks about all kinds of topics Beyond  parking and Beyond knee surgery so you  really want to somehow pull out only the  parts of your content that are relevant  to the patient's question so this is  another um a tricky part of this whole  rag architecture uh and the way that  works is that um you take all your  content and you break it into chunks or  these systems will break it into chunks  so chunk might be a paragraph of content  or a p or a couple paragraphs a page  something like that and then those um  chunks are sent to a large language  model could be the same one or a  different one and they are turned into a  vector  and uh so each each paragraph or each  chunk will have a  vector which is just is just a series of  numbers and that series of  numbers you can think of it as the  numeric representation of the essence of  that  paragraph and what's uh different about  these numbers just they're not random  numbers but paragraphs that talk about a  similar topic have close by numbers they  almost have the same vectors okay so in  addition to the uh it's a numera Zed  version of the paragraph but it's such  that similar paragraphs on similar  topics will have similar vectors will  have similar numbers so that means that  what happens is when um uh a user will  ask a question like do you have parking  let's  say then that is also sent to the llm in  real time right after the user asked the  question  that comes up with the vector as well  you could think of that as the question  vector and then what happens we do we do  a mathematical comparison real quick  between the vector of the question and  then the vectors of your content and  pick like the top five documents that  are closest to this question so do you  have parking will be a vector then you  have all your content and it's going to  try and find the five documents that  taught the most about parking basically  um and so it'll find those I don't know  what that is it'll find those documents  let's say uh from these it'll grab the  paragraphs associated with those  documents um and it'll use that  here so those will be the subset of your  content basically that is used as part  of the prompt before the prompt okay so  this whole uh concept is uh kind of  vectorizing your content uh typically  that then our storage in something  called a vector database which is  basically a representation of your  content in this numeric form and then  this system that you build this rag  system will uh take the question find  retrieve the most relevant content make  that as part of the prompt before the  prompt send that to the llm and then  you'll get a good response back actually  so it's a little bit confusing but um  but it's actually not that confusing um  uh I just made it more confusing by this  horrible uh horrible drawing but this  whole thing is um what is uh called rag  retrieval so you're retrieving the  relevant documents from your content  you're augmenting the generation process  so you're augmenting the lm's ability to  do generative AI based on the documents  that you retrieve so that's why it's  retrieval augmenting  generation okay so I hope that made  sense uh like I said this is a very  popular um solution pattern that I'm  seeing over and over again in fact the  majority of llm projects that I see are  this kind of thing using my content  packaging that up with an llm system to  create a kind of chat chpt like  experience for my employees or for my  customers for my users that kind of  thing and it works extremely well that's  why uh that's why it's so popular so I  hope that was interesting and  educational and made sense if you have  any questions please leave them for me  uh as part of the comments uh thank you  very much\", metadata={'source': 'u47GtXwePms', 'title': 'What is RAG? (Retrieval Augmented Generation)', 'description': 'Unknown', 'view_count': 39270, 'thumbnail_url': 'https://i.ytimg.com/vi/u47GtXwePms/hq720.jpg', 'publish_date': '2024-01-18 00:00:00', 'length': 697, 'author': 'Don Woodlock'})],\n 'url_list': ['https://www.youtube.com/watch?v=T-D1OfcDW1M&pp=ygUmV2hhdCBpcyByZXRyaWV2YWwgYXVnbWVudGVkIGdlbmVyYXRpb24%3D',\n  'https://www.youtube.com/watch?v=LAfrShnpVIk&pp=ygUmV2hhdCBpcyByZXRyaWV2YWwgYXVnbWVudGVkIGdlbmVyYXRpb24%3D',\n  'https://www.youtube.com/watch?v=u47GtXwePms&pp=ygUmV2hhdCBpcyByZXRyaWV2YWwgYXVnbWVudGVkIGdlbmVyYXRpb24%3D'],\n 'question': 'What is retrieval augmented generation',\n 'top_k': 3}"},"metadata":{}}]}]}